# 摘要
　　今天探討了如何將分類問題轉換為回歸問題，或反之亦然，以應對實際應用場景中的模糊地帶。回歸模型可透過離散化方法更準確地捕捉資料特性，而分類模型則能利用回歸生成的嵌入向量來加速預測任務。替代方案如多任務學習，則能結合回歸與分類兩者的優勢。此外，本文也分析了這種轉換可能帶來的代價，包括精度下降、資料量不足及標籤偏誤等風險。
# 重新定位
　　「重新定位」問題的作法是將分類問題以重新定位為回歸問題來處理，反之亦然。實際上在設計時，改變的是機器學習問題輸出的表現形式。
## 問題
　　我們在 [[[Day 2]　MLOps 是什麼？]] 討論到機器學習專案開發的核心是定義問題。這個專案是監督學習問題嗎？如果是的話，是分類還是回歸呢？即使我們確認為回歸問題，但這真的只能用回歸來完成嗎？如果用分類的方法來解決呢？
# 原理
　　之所以採用這種方法來解決問題，核心在於分類問題有時存在模糊地帶，而回歸問題在某些情況下，我們追求的並非一個精確的數值，而是一個合理的範圍。
## 解決方案
### 回歸
　　對於回歸問題而言，若僅依賴傳統回歸模型處理，當資料不符合常態分佈時，往往需要投入大量精力來設計模型，以精確捕捉這類分佈及其特徵。然而，在某些情況下，將結果劃分為一個範圍是可接受的，甚至在特定場景下，我們需要保留這種不確定性。此時，使用離散化方法反而更容易捕捉資料的特性。
　　除此之外，採用分組策略還能帶來額外的好處。雖然這可能會犧牲一部分精度（通常是在可接受的範圍內），但我們可以獲得機率密度函數 (PDF) 和後驗機率分佈等資訊，這些在傳統回歸模型中往往難以得到的額外資訊，能夠為決策提供更多依據。
### 分類
　　同樣地，這種思路也適用於分類問題。以推薦系統為例，若我們採用回歸模型來生成使用者偏好的嵌入向量 (embedding)，再通過相似度比較來推薦最符合使用者特性的電影，而不是單純訓練一個判斷使用者是否會喜歡某部電影的二元分類模型。這不僅能簡化問題本身的處理過程，還能顯著提升預測的即時性。
　　關於嵌入向量的相似度比對我們曾在 [[[Day 13]　設計模式：嵌入 (Embedding)]] 做過討論，另外我們也曾在 [[[Day 4]　建構 ML 系統的挑戰 — 資料品質]] 介紹過關於即時性的問題。
## 替代方案
### 多任務學習
　　我們也可以不在兩個方案中選擇，而是兩個都要。在神經網路中，常見的方法有硬參數共享和軟參數共享。
　　在硬參數共享中，所有輸出任務間會共享隱藏層的權重，類似的概念可以參考孿生神經網路 (Siamese Network)，這項技術我們曾在 [[[Day 13]　設計模式：嵌入 (Embedding)]] 提過，在那裡我們曾介紹過這是一種專門用來比較兩個輸入的相似度的神經網路模型。
　　軟參數共享的作法是透過正則化或某些方法來讓兩個模型擁有相似的權重，然而這兩個模型仍是獨立的，各自擁有自己的模型權重。
## 代價
### 精確度
　　將回歸問題轉換為分類問題進行處理，必然會對精確度產生一定影響。這是因為分類模型通常會將連續的輸出轉化為離散的範疇，從而導致預測結果無法達到回歸模型所能提供的精細度。
### 資料量
　　在改變問題的處理形式時，必須考慮到模型的精細程度往往與所需資料量成正比。以經驗法則為例，分類任務中的每個類別通常至少需要特徵數量的 10 倍樣本量，而回歸問題則需要約 50 倍。因此，在重新定義問題時，需要謹慎評估背景條件，確保有足夠的數據支持模型的複雜度。
### 標籤偏誤
　　由於這個方法需要自行定義標籤，所以需要注意是否會在無意中將標籤偏誤引入。關於人為引入的偏差我們曾在 [[[Day 5]　設計模式：公平的鏡頭 (Fairness Lens)]] 討論。
# 補充
## 機率密度函數 (Probability Density Function, PDF) 
　　機率密度函數用來描述連續隨機變數的概率分佈。它表示在某一區間內，隨機變數取某一值的概率密度。對於回歸問題來說，了解數據的機率密度函數可以幫助我們更好地理解資料的分佈情形，特別是當資料不是常態分佈時，PDF 能提供更多的細節資訊，進而影響模型的預測與設計。PDF 的應用特別適合在非線性回歸問題中，它能有效捕捉複雜的資料模式與不確定性。
## 分位數回歸／分量回歸 (Quantile Regression) 
　　分位數回歸是一種用來估計條件分佈中不同分位數的回歸技術，適合於非參數模型的情境。非參數指的是在建模過程中，不需要對資料的分佈形式做出假設，而是依據資料的實際特徵來進行回歸分析。這種方法不依賴於數據的特定分佈假設，使其在面對異質性較強或資料具有非標準分佈時特別有優勢。
　　通過分位數回歸，我們能夠不僅僅估計資料的均值，而是針對資料分佈的不同位置提供預測，這對於捕捉變量的上下波動、極端值，或評估風險和波動性等應用場景具有特殊意義。在這種情況下，分位數回歸作為一種非參數方法，提供了更多彈性與精細度，無需依賴傳統回歸對資料分佈的假設條件。
## 機率分佈回歸 (Probability Distribution Regression)
　　機率分佈回歸是一種基於輸出變量的機率分佈來進行回歸的技術。與傳統回歸模型專注於預測單一的期望值不同，這種方法同時考慮輸出的不確定性，通過估計條件機率分佈來全面描述輸出變量的行為。這種回歸方法特別適合應對具有高度不確定性或大幅波動的問題，例如金融風險評估、醫學預測和保險定價等領域。機率分佈回歸能提供更豐富的預測結果，讓決策者能在不確定性較高的情境中進行風險管理。
　　TensorFlow Probability 等框架提供了工具來實現這類問題的解決方案，相關資訊可以參考 [Regression with Probabilistic Layers in TensorFlow Probability](https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html)。
# 參考資料
- Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and MLOps CH3 Design Pattern: Reframing
- [分佈函數與機率密度函數](https://www.stat.nuk.edu.tw/cbme/math/statistic/sta2/s1_4/bud.html)
- [機率密度函數](https://zh.wikipedia.org/zh-tw/%E6%A9%9F%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B8)
- [預估外送時間 為...? — 分量回歸](https://ted21019.medium.com/%E9%A0%90%E4%BC%B0%E5%A4%96%E9%80%81%E6%99%82%E9%96%93-%E7%82%BA-%E5%88%86%E9%87%8F%E5%9B%9E%E6%AD%B8-dae2bab1c1d0)
- [Multi-Kernel Probability Distribution Regressions](https://lisc.mae.cornell.edu/LISCpresentations/Poster_IJCNN_2015_6.pdf)
- [Joint Probability Distribution Regression for Image Cropping](https://ieeexplore.ieee.org/document/10222223)
- [Regression with Probabilistic Layers in TensorFlow Probability](https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html)