# 摘要
　　今天討論了 one-hot 編碼在處理分類特徵上還存在無法處理分類變數間相依性的問題，也討論了嵌入 (Embedding) 可以怎麼解決這個問題，而這項技術又應如何進一步應用於自然語言處理這個典型的具高基數特徵的領域上。我們也介紹了特種方法在圖像／音訊處理上的應用，並簡單討論了可以如何藉由 embedding 引入相似度比較的概念來處裡分類問題。
# 嵌入
　　embedding 是一種資料表示法，這種做法的特色是將高維資料映射到低維的空間裡，從而以更密集的方式表示資料。
## 問題
### 稀疏的資料
　　延續我們在 [[[Day 12]　設計模式：被雜湊的特徵 (Hashed Feature)]] 對 one-hot 編碼的討論，在「高基數的分類特徵」這一節中，我們提到當分類特徵的基數極高時，使用 one-hot 編碼會導致維度急劇膨脹的問題；也解釋了為何在這種情況下 one-hot 編碼會使資料變得高度稀疏，並且進一步說明稀疏資料如何影響聚類算法及常見的機器學習模型的效能，使其難以產生有效結果。
　　因此，今天我們將不再對這個問題進行詳細討論，但這是我們需要使用 embedding 的核心原因之一。
### 分類變數間的相依性
　　one-hot 編碼將每個分類變數視為完全**獨立**的特徵，然而通常分類變數之間也存在某種關聯。我們在 [[[Day 12]　設計模式：被雜湊的特徵 (Hashed Feature)]] 的「冷啟動」中舉了一個關於航空公司的例子，對於家庭訂單，我們可能會期待三人小家庭與四人小家庭的表示方法相對於三人小家庭與頂客家庭會更接近，但當我們採用 one-hot 編碼時，這三個特徵之間的距離會完全相同。
## 解決方案
　　建立 Embedding 層，透過訓練獲得最適合各個類別的低維表示法。由於 Embedding 層的權重是在主模型訓練過程中同步學習的，因此這種方法能降低資料預處理的複雜度，它可以替代需要在資料預處理階段進行的聚類技術及降維方法（如主成分分析 (PCA)）。
### 自然語言處理
　　因為文本的詞彙量通常數以萬計，因此對每個單字進行 one-hot 編碼非常不切實際，而且在處理自然語言時，我們的目標之一就是捕捉詞彙跟詞彙之間的關聯，這也是 one-hot 編碼無法達成的。
1. 先建立單字表
2. 利用單字表為文本進行初步編碼（文本 > 數值序列）
3. 建立一個神經網路模型，並在其中加上 Embedding 層，其中 Embedding 層的輸入維度設為詞彙表的大小
　　書中的範例是在 Embedding 層後加入一層用於取 Embedding 層輸出的平均值，這麼做的好處是能得到一個簡單的、統整了整體單字資訊的結果，不過也將因此失去單字順序的資訊。
### 圖像／音訊處理
　　我們都知道圖像、音訊的資料已經相當密集了，這麼密集的資料為什麼還需要會用到 embedding 呢？原因是， embedding 可以用於表示輸入的相關性，此外，透過較低維度的表示也可以簡化相關計算。
　　圖像 Embedding 層的建立方式是先找到已使用大型圖像資料庫進行訓練的預訓練模型，然後移除最後的 softmax 層。這個方法的原理可以拆解成兩個部分來看，首先是從卷積神經網路的架構上來說，模型會在這一層之前產生一個特徵向量 (eigenvector)，因此移除最末層後，這個特徵向量就可以視為這張圖的 embedding。
　　其次，為什麼要使用已針對大型圖像資料庫進行訓練的預訓練模型來產生 embedding 呢？這是運用了遷移學習 (Transfer Learning) 的概念，經過大型圖像資料庫訓練的模型已經學習到多樣化的圖像特徵，透過這樣的模型，我們可以將這些學習成果轉移過來，從而獲得更穩定且具代表性的資料表示。
### 相似度比較
　　當我們獲得一個嵌入於低維空間的向量後，我們可以透過**內積**來計算不同向量之間的相似度。內積值越大，表示兩個向量在空間中的方向越相近，代表它們對應的資料（例如圖像、文本）在特徵空間中的相似度也越高。
　　這種方法廣泛應用於許多任務中，如圖像檢索、文本相似度計算、推薦系統等，能夠有效地幫助我們進行相似項目的比對和篩選。此外，餘弦相似度、歐幾里得距離等方法也可以與嵌入向量結合，進行更精細的相似度衡量。
　　相關技術也可以參考孿生神經網路 (Siamese Network) ，它是一種專門用來比較兩個輸入的相似度的神經網路模型。
## 代價
### 遺失資訊
　　所有的降維技術都會面臨一個問題：遺失資訊，Embedding 也不例外，因此我們也將面臨取捨的問題，太大的輸出維度會導致資料稀疏，太小則因損失過多資訊，而將導致難以真正表示資料。
　　針對設定輸出維度來說，有一些經驗法則可以參考：
1. 元素類別數量的四次方根
2. 元素類別數量的平方根的 1.6 倍，並且不少於 600 個

　　我們可以優先在這個範圍內尋找最好的輸出維度大小。
### 再現性
　　我們曾在 [[[Day 7]　建構 ML 系統的挑戰 — 再現性]] 中討論過再現性的問題。今天介紹的方法所產生的 embedding 會受到模型訓練過程的影響，這意味著每次訓練時，由於隨機梯度下降和 Dropout 層的存在，生成的 embedding 可能會有所不同。因此，在每次訓練模型時，資料表示（embedding）都會隨之改變，進而使得再現訓練結果變得困難。
　　為了簡化這部分的問題，一個有效的做法是使用 Feature Store。這種方法的特點在於將特徵工程與模型訓練分離，但這並不意味著我們不再利用模型訓練來獲得 embedding，而是將生成的資料表示（embedding）儲存起來，從而在後續比較不同模型時能直接使用，確保一致性，並減少再現性問題。
# 其他方案
- Autoencoder：透過編碼器與解碼器結構學習壓縮資料的低維表示，常用於無監督學習中的降維和特徵提取。
- Word2Vec, BERT, NNLM, GLoVE：這些模型同時捕捉文字的語義及上下文關係，用於生成具語義理解的文字嵌入表示。
# 補充
## one-hot 編碼的問題
　　結合 [[[Day 12]　設計模式：被雜湊的特徵 (Hashed Feature)]]，我們目前已經討論了 one-hot 編碼存在的三個問題：詞彙表不完整、高基數的分類特徵、分類變數間的相依性。
## Eigenspace Learning
　　因為在「圖像／音訊處理」這一小節中提到了特徵向量，因此在這裡補充關於特徵空間學習的一些範例。在應用上，可以參考這兩篇論文：[Multi-Eigenspace Learning for Video-Based Face Recognition](https://link.springer.com/chapter/10.1007/978-3-540-74549-5_20) 和 [Robust Malware Detection for Internet of (Battlefield) Things Devices Using Deep Eigenspace Learning](https://ieeexplore.ieee.org/document/8302863)。第一篇討論了利用特徵空間、特徵向量的機器學習可以怎麼應用在影片中的人臉辨識上，第二篇則是在惡意程式偵測上的應用，其中後者取得的 embedding 方法我個人覺得相當有趣，從二進位樣本到產生圖 (graph) 表示的部分暫且不提，它從樣本的圖取得最終輸入的方法是藉由結合圖的第一特徵向量、第一特徵值、第二特徵向量、第二特徵值來取得 embedding。
# 參考資料
- Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and MLOps CH2 Design Pattern:  Embedding, CH6 Design Pattern:  Feature Store
- [word2vec原理及gensim中word2vec的使用](https://www.cnblogs.com/zjuhaohaoxuexi/p/15170991.html "发布于 2021-08-21 23:43")
- [Bert代码实现+论文地址+讲解](https://blog.csdn.net/qq_45056135/article/details/123186373)